# IRF-Calc v0.0.2 â€” Philosophical Algorithms for AI

```python
# IRF-Calc v0.0.2 â€” Philosophical Algorithms for AI
# REx Engine / ASDP í†µí•©ìš© Helm values (Optimized)

irfCalc:
  enabled: true
  id: "irf_calc_philosophical_ai"
  version: "0.0.2"
  mode: "conservative"        # conservative | balanced | exploratory
  description: >
    IRF-Calc v0.0.2 is an integrated philosophical reasoning algorithm for AI.
    It quantifies reasoning quality over six components (M,A,D,I,F,P),
    controls drift and calibration, and adds a DR3 decision protocol layer
    for conservative final choice.

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 1. ì»´í¬ë„ŒíŠ¸ ì •ì˜ (0~1 ì‹¤ìˆ˜ ìŠ¤ì½”ì–´)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  components:
    M:
      id: "M"
      name: "methodic_doubt"
      desc: "ì „ì œ ì •í™” í’ˆì§ˆ"
    A:
      id: "A"
      name: "abduction"
      desc: "ê°€ì„¤ ìƒì„± í’ˆì§ˆ"
    D:
      id: "D"
      name: "deduction"
      desc: "ì—°ì—­ ì „ê°œ í’ˆì§ˆ"
    I:
      id: "I"
      name: "induction"
      desc: "ê²½í—˜/ë°ì´í„° ì •í•©ì„±"
    F:
      id: "F"
      name: "falsification"
      desc: "ë°˜ì¦ ë¶„ì„ ì—„ê²©ì„±"
    P:
      id: "P"
      name: "paradigm"
      desc: "íŒ¨ëŸ¬ë‹¤ì„ ì¸ì‹/ì •ë ¬ ìˆ˜ì¤€"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 2. IRF Scoring (ì—°ì† ì ìˆ˜ / ì •ìˆ˜ ì ìˆ˜)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  scoring:
    epsilon: 0.001
    formulaContinuous: |
      # q_i = (M_i, A_i, D_i, I_i, F_i, P_i) in [0,1]^6
      # s_i = IRF_score(h_i)
      # s_i = (Î _k (epsilon + q_{i,k}))^(1/6)
    formulaInteger:
      component: |
        # R_{i,k} in {0..10}
        # R_{i,k} = round(10 * q_{i,k})
      global: |
        # R_i in {0..100}
        # R_i = round(100 * s_i)

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 3. Thresholds (ì² í•™ì  í•©ê²©ì„ )
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  thresholds:
    # ìš´ì˜ ëª¨ë“œì— ë”°ë¼ globalMin ì¡°ì • (0.0.2 ìµœì í™”)
    profiles:
      conservative:
        globalMin: 0.80
      balanced:
        globalMin: 0.78
      exploratory:
        globalMin: 0.75
    # ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°’ (í…œí”Œë¦¿ì—ì„œ .Values.irfCalc.thresholds.effective.globalMin ì‚¬ìš©)
    effective:
      globalMin: 0.80        # default: conservative
      componentMin:
        M: 0.70
        A: 0.70
        D: 0.70
        I: 0.70
        F: 0.70
        P: 0.70

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 4. Drift ì œì–´ (IRF-DriftLock)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  drift:
    jsd:
      enabled: true
      name: "drift_jsd_irf"
      description: "ë¶„í¬ ê¸°ë°˜ IRF score drift (Jensenâ€“Shannon, normalized [0,1])"
      max: 0.06
      alertLevels:
        warning: 0.04
        critical: 0.06
      formula: |
        # p_i^(t) = s_i^(t) / Î£_j s_j^(t)
        # drift_jsd_irf = JSD(p^(t1), p^(t2)) / log(2)
    l2:
      enabled: true
      name: "drift_l2_irf"
      description: "ì»´í¬ë„ŒíŠ¸ í‰ê·  ë²¡í„° ê¸°ë°˜ L2 drift"
      max: 0.04
      alertLevels:
        warning: 0.03
        critical: 0.04
      formula: |
        # q_bar^(t) = mean_i q_i^(t)
        # drift_l2_irf = || q_bar^(t1) - q_bar^(t2) ||_2

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 5. Calibration Gate (ë ˆí¼ëŸ°ìŠ¤ ìƒê´€ë„)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  calibration:
    enabled: true
    name: "corr_irf_ref"
    description: >
      IRF score vs reference benchmark score (from arxiv-derived D/I/A + abduction + Socratic tasks)
      Pearson correlation coefficient.
    min: 0.90
    alertLevels:
      warning: 0.92
      critical: 0.90
    formula: |
      # corr_irf_ref = Corr(y_t, y_hat_t)
      # y_t      : reference (human/paper) score for task t
      # y_hat_t  : IRF-based score

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 6. Inference Controller (ë£¨í”„ ì„¤ì •)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  inference:
    maxCycles: 2
    stopConditions:
      - "exists(h_i: s_i >= thresholds.effective.globalMin AND all(q_{i,k} >= thresholds.effective.componentMin[k]))"
      - "drift_jsd_irf <= drift.jsd.max"
      - "drift_l2_irf <= drift.l2.max"
    exportAsEnv: true
    envPrefix: "IRF_LOOP_"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 7. Decision Layer (DR3_decision_protocol)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  decision:
    protocol: "DR3_decision_protocol"
    description: "Deductive + Inductive fusion with 3 candidate strategies and conservative final choice."
    axes:
      - "realism"
      - "stability"
      - "conservative_rationality"
    weights:
      realism: 0.4
      stability: 0.4
      conservative_rationality: 0.2
    minAxisScore: 0.6
    candidates: 3
    mapping:
      deductiveSources: ["D", "F"]     # ì—°ì—­Â·ë°˜ì¦
      inductiveSources: ["A", "I"]     # ê°€ì„¤Â·ê·€ë‚©
      metaSources: ["M", "P"]          # ì „ì œÂ·íŒ¨ëŸ¬ë‹¤ì„ (ë©”íƒ€ ê³„ì¸µ)
    selectionRule: "select argmax_j score_j s.t. all(axis_score_j >= minAxisScore)"
    fallbackRule: "abstain_and_request_more_evidence"
    exportAsEnv: true
    envPrefix: "IRF_DR3_"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 8. REx Engine ì—°ë™
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  rex:
    gate:
      enabled: true
      # IRF ì ìˆ˜ ê¸°ë°˜ ìŠ¹ì¸/ë³´ë¥˜/ê±°ë¶€ ì •ì±…
      acceptIf:
        irfScoreMin: 0.80         # thresholds.effective.globalMinì™€ sync
        irfIntegerMin: 80
      holdIf:
        driftJsdOverWarning: 0.04
        driftL2OverWarning: 0.03
      rejectIf:
        driftJsdOver: 0.06
        driftL2Over: 0.04
        corrRefBelow: 0.90
    prometheus:
      enabled: true
      metricPrefix: "rex_irf"
      metrics:
        - name: "rex_irf_score"
          help: "IRF global continuous score s_i"
          type: "gauge"
        - name: "rex_irf_score_int"
          help: "IRF integer score R_i (0..100)"
          type: "gauge"
        - name: "rex_irf_drift_jsd"
          help: "IRF drift JSD normalized"
          type: "gauge"
        - name: "rex_irf_drift_l2"
          help: "IRF drift L2"
          type: "gauge"
        - name: "rex_irf_corr_ref"
          help: "IRF vs reference correlation"
          type: "gauge"
        - name: "rex_irf_dr3_realism"
          help: "DR3 realism axis score of selected candidate"
          type: "gauge"
        - name: "rex_irf_dr3_stability"
          help: "DR3 stability axis score of selected candidate"
          type: "gauge"
        - name: "rex_irf_dr3_conservative_rationality"
          help: "DR3 conservative rationality axis score"
          type: "gauge"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 9. ASDP ì—°ë™
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  asdp:
    sovereignDef:
      id: "irf_integrated_reasoning_framework"
      version: "0.0.2"
      registryRef: "asdp/definitions/irf_integrated_reasoning_framework.v0.0.2.yaml"
    routing:
      domain: "research-philosophy"
      tags:
        - "irf"
        - "philosophical-reasoning"
        - "ded-ind-abd"
        - "driftlock"
        - "dr3-decision"
    policyBinding:
      evidenceFirst: true
      abstainOverFabricate: true
      enforceDriftLimits: true
      enforceCalibrationGate: true
      enforceDecisionLayer: true
```

```json
irf-df-mini-toolkit/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ golden_examples/
â”‚   â”œâ”€â”€ M/ (4 files)
â”‚   â”œâ”€â”€ D/ (4 files)
â”‚   â””â”€â”€ F/ (4 files)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ annotation_guide.md
â”‚   â””â”€â”€ calibration_quiz.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_pilot_data.py        â† ìƒˆë¡œ ë§Œë“  ê²ƒ
â”‚   â”œâ”€â”€ compute_iaa.py                â† ìƒˆë¡œ ë§Œë“  ê²ƒ
â”‚   â””â”€â”€ validate_annotations.py       (ë‚˜ì¤‘ì— ì¶”ê°€)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ label_studio_M_template.xml
â”‚   â”œâ”€â”€ label_studio_D_template.xml
â”‚   â””â”€â”€ label_studio_F_template.xml
â””â”€â”€ data/
    â”œâ”€â”€ pilot/ (ìˆ˜ì§‘ëœ ë°ì´í„°)
    â””â”€â”€ annotations/ (ë¼ë²¨ë§ ê²°ê³¼)
```

```python
{
  "irf_id": "irf_calc_philosophical_ai",
  "irf_version": "0.0.2",
  "description": "IRF-Calc v0.0.2 â€“ Philosophical Algorithms for AI (LLM fine-tuning + inference controller + DR3 decision layer)",
  "base_model": "your-llm-name-here",
  "target_role": "IRF-philosophical-reasoner",

  "components": ["M", "A", "D", "I", "F", "P"],

  "scoring": {
    "epsilon": 0.001,
    "global_formula": "s_i = (Î _k (epsilon + q_{i,k}))^(1/6)",
    "integer_component": "R_{i,k} = round(10 * q_{i,k})",
    "integer_global": "R_i = round(100 * s_i)",
    "thresholds": {
      "global_min": 0.78,
      "component_min": {
        "M": 0.70,
        "A": 0.70,
        "D": 0.70,
        "I": 0.70,
        "F": 0.70,
        "P": 0.70
      }
    }
  },

  "drift_control": {
    "jsd_max": 0.06,
    "l2_max": 0.04
  },

  "calibration": {
    "corr_ref_min": 0.90,
    "note": "Use arxiv-derived D/I/A + abduction + Socratic benchmarks as reference tasks."
  },

  "training": {
    "strategy": "stagewise_instruction_tuning",
    "stages": [
      {
        "name": "premise_purification",
        "id": "M",
        "task_type": "premise_extraction_and_critique",
        "datasets": [
          "custom:irf_premise_purification_v1"
        ],
        "input_format": {
          "fields": ["question", "context"],
          "output_fields": ["premise_list", "suspect_flags", "M_score"]
        },
        "loss": "cross_entropy + regression(M_score)",
        "metrics": ["M_score_mae", "M_score_corr"],
        "max_steps": 30000
      },
      {
        "name": "hypothesis_generation",
        "id": "A",
        "task_type": "abductive_hypothesis_generation",
        "datasets": [
          "arxiv:causejudger_causelogics",
          "arxiv:gear_abduction",
          "custom:irf_abduction_v1"
        ],
        "input_format": {
          "fields": ["scenario", "observations"],
          "output_fields": ["hypotheses", "plausibility_scores", "A_score"]
        },
        "loss": "sequence_ce + regression(A_score)",
        "metrics": ["A_score_mae", "A_score_corr"],
        "max_steps": 40000
      },
      {
        "name": "deductive_expansion",
        "id": "D",
        "task_type": "logical_consequence_derivation",
        "datasets": [
          "logic:theorem_proving_synthetic",
          "custom:irf_deduction_v1"
        ],
        "input_format": {
          "fields": ["premises", "hypothesis"],
          "output_fields": ["deductive_trace", "D_score"]
        },
        "loss": "sequence_ce + regression(D_score)",
        "metrics": ["D_score_mae", "D_score_corr"],
        "max_steps": 40000
      },
      {
        "name": "inductive_check",
        "id": "I",
        "task_type": "inductive_consistency_check",
        "datasets": [
          "arxiv:inductive_linguistic_reasoning",
          "custom:irf_induction_v1"
        ],
        "input_format": {
          "fields": ["hypothesis", "observations"],
          "output_fields": ["fit_score", "counterexamples", "I_score"]
        },
        "loss": "regression(I_score)",
        "metrics": ["I_score_mae", "I_score_corr"],
        "max_steps": 30000
      },
      {
        "name": "falsification_gate",
        "id": "F",
        "task_type": "falsification_labeling",
        "datasets": [
          "custom:irf_falsification_v1"
        ],
        "input_format": {
          "fields": ["hypothesis", "evidence"],
          "output_fields": ["label", "F_score"]
        },
        "loss": "classification(label) + regression(F_score)",
        "metrics": ["f1_macro", "F_score_corr"],
        "max_steps": 25000
      },
      {
        "name": "paradigm_alignment",
        "id": "P",
        "task_type": "paradigm_consistency_and_shift",
        "datasets": [
          "custom:irf_paradigm_v1",
          "arxiv:socratic_llm_datasets"
        ],
        "input_format": {
          "fields": ["current_paradigm", "new_evidence"],
          "output_fields": ["paradigm_notes", "shift_type", "P_score"]
        },
        "loss": "sequence_ce + regression(P_score)",
        "metrics": ["P_score_mae", "P_score_corr"],
        "max_steps": 25000
      }
    ],
    "joint_finetune": {
      "enabled": true,
      "description": "Final multi-task joint tuning over all IRF components.",
      "sampling_strategy": "proportional_to_dataset_size",
      "max_steps": 60000
    }
  },

  "inference_controller": {
    "loop": {
      "max_cycles": 2,
      "stop_conditions": [
        "exists(h_i: s_i >= thresholds.global_min AND all(q_{i,k} >= thresholds.component_min[k]))",
        "drift_jsd_irf <= drift_control.jsd_max",
        "drift_l2_irf <= drift_control.l2_max"
      ]
    },
    "prompts": {
      "premise_purification": "List all explicit and implicit premises in the question, mark doubtful ones, and output M_score in [0,1].",
      "hypothesis_generation": "Generate 3-5 plausible hypotheses with pros/cons and output A_score in [0,1].",
      "deductive_expansion": "For each hypothesis, derive step-by-step logical consequences and output D_score in [0,1].",
      "inductive_check": "Given data/facts, estimate inductive fit and output I_score in [0,1].",
      "falsification_gate": "Search for killer counterexamples, label each hypothesis as accept/hold/reject, and output F_score in [0,1].",
      "paradigm_alignment": "Describe how the results align/conflict with current paradigm and output P_score in [0,1]."
    }
  },

  "decision_layer": {
    "name": "DR3_decision_protocol",
    "description": "Deductive + Inductive fusion with 3 candidate strategies and conservative final choice.",
    "axes": [
      "realism",
      "stability",
      "conservative_rationality"
    ],
    "weights": {
      "realism": 0.4,
      "stability": 0.4,
      "conservative_rationality": 0.2
    },
    "min_axis_score": 0.6,
    "candidates": 3,
    "mapping": {
      "deductive_sources": ["D", "F"],
      "inductive_sources": ["A", "I"],
      "meta_sources": ["M", "P"]
    },
    "selection_rule": "select argmax_j score_j s.t. all(axis_score_j >= min_axis_score)",
    "fallback_rule": "if no candidate satisfies constraints, abstain_and_request_more_evidence",
    "notes": "D,FëŠ” ì—°ì—­/ë°˜ì¦ ê¸°ë°˜, A,IëŠ” ê°€ì„¤/ê·€ë‚© ê¸°ë°˜, M,PëŠ” ì „ì œ/íŒ¨ëŸ¬ë‹¤ì„ ê¸°ë°˜ ë©”íƒ€ ì •ë³´ë¡œ ì‚¬ìš©í•œë‹¤."
  }
}
```

```json
#!/usr/bin/env python3
"""
IRF-DF-Mini Pilot Data Collection Script

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” GitHub APIë¥¼ ì‚¬ìš©í•˜ì—¬ code review ì½”ë©˜íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³ ,
M/D/F ì»´í¬ë„ŒíŠ¸ í•™ìŠµì— ì í•©í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

Requirements:
    pip install requests tqdm

Usage:
    export GITHUB_TOKEN="your_github_personal_access_token"
    python collect_pilot_data.py --component M --count 30
"""

import os
import json
import time
import argparse
from typing import List, Dict, Optional
from datetime import datetime, timedelta

import requests
from tqdm import tqdm

class GitHubDataCollector:
    """GitHub APIë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, token: Optional[str] = None):
        self.token = token or os.getenv("GITHUB_TOKEN")
        if not self.token:
            raise ValueError("GITHUB_TOKEN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ tokenì„ ì „ë‹¬í•˜ì„¸ìš”")
        
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"token {self.token}",
            "Accept": "application/vnd.github.v3+json"
        })
        self.base_url = "https://api.github.com"
    
    def search_repos(self, language: str = "python", min_stars: int = 1000, 
                     max_results: int = 10) -> List[str]:
        """ì¸ê¸° ìˆëŠ” ì €ì¥ì†Œ ê²€ìƒ‰"""
        query = f"language:{language} stars:>{min_stars}"
        url = f"{self.base_url}/search/repositories"
        params = {"q": query, "sort": "stars", "per_page": max_results}
        
        response = self.session.get(url, params=params)
        response.raise_for_status()
        
        repos = response.json()["items"]
        return [repo["full_name"] for repo in repos]
    
    def get_pull_request_reviews(self, repo: str, pr_number: int) -> List[Dict]:
        """íŠ¹ì • PRì˜ ë¦¬ë·° ì½”ë©˜íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/repos/{repo}/pulls/{pr_number}/comments"
        response = self.session.get(url)
        
        if response.status_code != 200:
            return []
        
        return response.json()
    
    def search_prs_with_reviews(self, repo: str, max_prs: int = 50) -> List[Dict]:
        """ë¦¬ë·°ê°€ ìˆëŠ” PR ê²€ìƒ‰"""
        # ìµœê·¼ 6ê°œì›” ë‚´ merged PR
        since_date = (datetime.now() - timedelta(days=180)).isoformat()
        url = f"{self.base_url}/repos/{repo}/pulls"
        params = {
            "state": "closed",
            "sort": "updated",
            "direction": "desc",
            "per_page": max_prs
        }
        
        response = self.session.get(url, params=params)
        if response.status_code != 200:
            return []
        
        prs = response.json()
        
        # Mergedì´ê³  ë¦¬ë·°ê°€ ìˆëŠ” PRë§Œ í•„í„°ë§
        result = []
        for pr in prs:
            if pr.get("merged_at"):
                result.append({
                    "repo": repo,
                    "pr_number": pr["number"],
                    "title": pr["title"],
                    "merged_at": pr["merged_at"]
                })
        
        return result
    
    def collect_m_data(self, count: int = 30) -> List[Dict]:
        """M (Premise Purification) ë°ì´í„° ìˆ˜ì§‘
        
        Code review ì½”ë©˜íŠ¸ì—ì„œ 'ì´ ë³€ê²½ì€ Xë¥¼ ê°€ì •í•œë‹¤'ëŠ” ë¥˜ì˜ ì½”ë©˜íŠ¸ ìˆ˜ì§‘
        """
        print(f"ğŸ” M ì»´í¬ë„ŒíŠ¸ ë°ì´í„° {count}ê°œ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ: assume, premise, given that ë“±
        keywords = ["assumes", "assuming", "premise", "given that", "this relies on"]
        
        repos = self.search_repos(language="python", min_stars=5000, max_results=5)
        print(f"ğŸ“¦ ê²€ìƒ‰ ëŒ€ìƒ ì €ì¥ì†Œ: {', '.join(repos)}")
        
        collected = []
        
        for repo in tqdm(repos, desc="ì €ì¥ì†Œ íƒìƒ‰"):
            prs = self.search_prs_with_reviews(repo, max_prs=20)
            
            for pr in prs:
                if len(collected) >= count:
                    break
                
                reviews = self.get_pull_request_reviews(repo, pr["pr_number"])
                
                for review in reviews:
                    body = review.get("body", "").lower()
                    
                    # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì½”ë©˜íŠ¸ë§Œ
                    if any(kw in body for kw in keywords):
                        collected.append({
                            "component": "M",
                            "source": "github_pr_review",
                            "repo": repo,
                            "pr_number": pr["pr_number"],
                            "pr_title": pr["title"],
                            "question": f"This code change: {pr['title']}",
                            "context": review["body"],
                            "metadata": {
                                "pr_url": f"https://github.com/{repo}/pull/{pr['pr_number']}",
                                "collected_at": datetime.now().isoformat()
                            }
                        })
                        
                        if len(collected) >= count:
                            break
                
                # Rate limiting ë°©ì§€
                time.sleep(0.5)
            
            if len(collected) >= count:
                break
        
        print(f"âœ… M ë°ì´í„° {len(collected)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return collected
    
    def collect_d_data(self, count: int = 40) -> List[Dict]:
        """D (Deduction) ë°ì´í„° ìˆ˜ì§‘
        
        Code reviewì—ì„œ ë…¼ë¦¬ì  ë‹¨ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ì½”ë©˜íŠ¸ ìˆ˜ì§‘
        """
        print(f"ğŸ” D ì»´í¬ë„ŒíŠ¸ ë°ì´í„° {count}ê°œ ìˆ˜ì§‘ ì‹œì‘...")
        
        keywords = ["because", "therefore", "thus", "this means", "which leads to",
                   "step by step", "first", "second", "then"]
        
        repos = self.search_repos(language="python", min_stars=5000, max_results=5)
        collected = []
        
        for repo in tqdm(repos, desc="ì €ì¥ì†Œ íƒìƒ‰"):
            prs = self.search_prs_with_reviews(repo, max_prs=20)
            
            for pr in prs:
                if len(collected) >= count:
                    break
                
                reviews = self.get_pull_request_reviews(repo, pr["pr_number"])
                
                for review in reviews:
                    body = review.get("body", "")
                    
                    # ê¸¸ì´ê°€ ì ë‹¹í•˜ê³  (100-500ì) í‚¤ì›Œë“œ í¬í•¨
                    if 100 <= len(body) <= 500 and any(kw in body.lower() for kw in keywords):
                        collected.append({
                            "component": "D",
                            "source": "github_pr_review",
                            "repo": repo,
                            "pr_number": pr["pr_number"],
                            "premises": f"Code change in {pr['title']}",
                            "hypothesis": "This change is safe/correct",
                            "response": body,
                            "metadata": {
                                "pr_url": f"https://github.com/{repo}/pull/{pr['pr_number']}",
                                "collected_at": datetime.now().isoformat()
                            }
                        })
                        
                        if len(collected) >= count:
                            break
                
                time.sleep(0.5)
            
            if len(collected) >= count:
                break
        
        print(f"âœ… D ë°ì´í„° {len(collected)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return collected
    
    def collect_f_data(self, count: int = 30) -> List[Dict]:
        """F (Falsification) ë°ì´í„° ìˆ˜ì§‘
        
        'What if', 'edge case', 'what about' ë¥˜ì˜ ë°˜ë¡€ ì œì‹œ ì½”ë©˜íŠ¸ ìˆ˜ì§‘
        """
        print(f"ğŸ” F ì»´í¬ë„ŒíŠ¸ ë°ì´í„° {count}ê°œ ìˆ˜ì§‘ ì‹œì‘...")
        
        keywords = ["what if", "edge case", "what about", "but what happens",
                   "counterexample", "fails when", "doesn't work", "breaks"]
        
        repos = self.search_repos(language="python", min_stars=5000, max_results=5)
        collected = []
        
        for repo in tqdm(repos, desc="ì €ì¥ì†Œ íƒìƒ‰"):
            prs = self.search_prs_with_reviews(repo, max_prs=20)
            
            for pr in prs:
                if len(collected) >= count:
                    break
                
                reviews = self.get_pull_request_reviews(repo, pr["pr_number"])
                
                for review in reviews:
                    body = review.get("body", "")
                    
                    if any(kw in body.lower() for kw in keywords):
                        collected.append({
                            "component": "F",
                            "source": "github_pr_review",
                            "repo": repo,
                            "pr_number": pr["pr_number"],
                            "hypothesis": f"This change works correctly: {pr['title']}",
                            "evidence": "Code review discussion",
                            "response": body,
                            "metadata": {
                                "pr_url": f"https://github.com/{repo}/pull/{pr['pr_number']}",
                                "collected_at": datetime.now().isoformat()
                            }
                        })
                        
                        if len(collected) >= count:
                            break
                
                time.sleep(0.5)
            
            if len(collected) >= count:
                break
        
        print(f"âœ… F ë°ì´í„° {len(collected)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return collected

def generate_synthetic_data(component: str, count: int) -> List[Dict]:
    """GitHub ë°ì´í„°ê°€ ë¶€ì¡±í•  ê²½ìš° synthetic ë°ì´í„° ìƒì„±
    
    ë‚˜ì¤‘ì— GPT-4ë¡œ ìƒì„± + human verification í•˜ë©´ ë¨
    """
    print(f"âš ï¸  {component} ì»´í¬ë„ŒíŠ¸: GitHubì—ì„œ ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ’¡ ëŒ€ì•ˆ: GPT-4ë¡œ synthetic ë°ì´í„° ìƒì„± í›„ expert verification ê¶Œì¥")
    
    return [
        {
            "component": component,
            "source": "synthetic_placeholder",
            "note": f"ì´ {count}ê°œ ë°ì´í„°ëŠ” GPT-4 ìƒì„± + expert verification í•„ìš”",
            "generated_at": datetime.now().isoformat()
        }
    ]

def main():
    parser = argparse.ArgumentParser(description="IRF-DF-Mini Pilot ë°ì´í„° ìˆ˜ì§‘")
    parser.add_argument("--component", choices=["M", "D", "F", "all"], default="all",
                       help="ìˆ˜ì§‘í•  ì»´í¬ë„ŒíŠ¸")
    parser.add_argument("--count", type=int, default=None,
                       help="ìˆ˜ì§‘í•  ë°ì´í„° ê°œìˆ˜ (ê¸°ë³¸: M=30, D=40, F=30)")
    parser.add_argument("--output", default="pilot_data",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: pilot_data/)")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output, exist_ok=True)
    
    try:
        collector = GitHubDataCollector()
        
        if args.component == "all" or args.component == "M":
            m_count = args.count if args.count else 30
            m_data = collector.collect_m_data(m_count)
            
            # ë¶€ì¡±í•˜ë©´ synthetic placeholder
            if len(m_data) < m_count:
                m_data.extend(generate_synthetic_data("M", m_count - len(m_data)))
            
            with open(f"{args.output}/M_pilot.json", "w", encoding="utf-8") as f:
                json.dump(m_data, f, indent=2, ensure_ascii=False)
        
        if args.component == "all" or args.component == "D":
            d_count = args.count if args.count else 40
            d_data = collector.collect_d_data(d_count)
            
            if len(d_data) < d_count:
                d_data.extend(generate_synthetic_data("D", d_count - len(d_data)))
            
            with open(f"{args.output}/D_pilot.json", "w", encoding="utf-8") as f:
                json.dump(d_data, f, indent=2, ensure_ascii=False)
        
        if args.component == "all" or args.component == "F":
            f_count = args.count if args.count else 30
            f_data = collector.collect_f_data(f_count)
            
            if len(f_data) < f_count:
                f_data.extend(generate_synthetic_data("F", f_count - len(f_data)))
            
            with open(f"{args.output}/F_pilot.json", "w", encoding="utf-8") as f:
                json.dump(f_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ê²°ê³¼ëŠ” {args.output}/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë¨")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ í™•ì¸")
        print("2. Synthetic placeholderë¥¼ GPT-4ë¡œ ìƒì„±")
        print("3. Expertê°€ ëª¨ë“  ë°ì´í„° ê²€í† ")
        print("4. Label Studioë¡œ import")
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        print("\në¬¸ì œ í•´ê²° íŒ:")
        print("1. GITHUB_TOKENì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("2. Tokenì— 'repo' scope ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
        print("3. Rate limitì— ê±¸ë ¸ë‹¤ë©´ 1ì‹œê°„ í›„ ì¬ì‹œë„")

if __name__ == "__main__":
    main()
```

```json
#!/usr/bin/env python3
"""
Inter-Annotator Agreement (IAA) ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸

Cohen's Kappa, Fleiss' Kappa, Krippendorff's Alphaë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

Requirements:
    pip install numpy pandas scikit-learn statsmodels

Usage:
    python compute_iaa.py --input annotations.json --output iaa_report.json
"""

import json
import argparse
from typing import List, Dict, Tuple
from collections import defaultdict

import numpy as np
import pandas as pd
from sklearn.metrics import cohen_kappa_score
from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters

class IAACalculator:
    """Inter-Annotator Agreement ê³„ì‚°ê¸°"""
    
    def __init__(self, annotations: List[Dict]):
        """
        annotations: [
            {
                "item_id": "M_000001",
                "annotator": "annotator_1",
                "label": "A_better",  # or M_score, etc.
                "component": "M"
            },
            ...
        ]
        """
        self.annotations = annotations
        self.df = pd.DataFrame(annotations)
    
    def cohen_kappa_pairwise(self, annotator1: str, annotator2: str, 
                             component: str = None) -> Dict:
        """ë‘ annotator ê°„ì˜ Cohen's Kappa ê³„ì‚°"""
        
        df = self.df.copy()
        if component:
            df = df[df["component"] == component]
        
        # ë‘ annotatorì˜ annotationì´ ëª¨ë‘ ìˆëŠ” itemë§Œ ì„ íƒ
        items_a1 = set(df[df["annotator"] == annotator1]["item_id"])
        items_a2 = set(df[df["annotator"] == annotator2]["item_id"])
        overlap_items = items_a1 & items_a2
        
        if len(overlap_items) == 0:
            return {
                "annotators": [annotator1, annotator2],
                "component": component,
                "overlap_count": 0,
                "kappa": None,
                "error": "No overlapping items"
            }
        
        # ê° annotatorì˜ ë¼ë²¨ ì¶”ì¶œ
        labels_a1 = []
        labels_a2 = []
        
        for item_id in sorted(overlap_items):
            label1 = df[(df["item_id"] == item_id) & 
                       (df["annotator"] == annotator1)]["label"].values[0]
            label2 = df[(df["item_id"] == item_id) & 
                       (df["annotator"] == annotator2)]["label"].values[0]
            labels_a1.append(label1)
            labels_a2.append(label2)
        
        # Cohen's Kappa ê³„ì‚°
        kappa = cohen_kappa_score(labels_a1, labels_a2)
        
        # Agreement rate
        agreement_rate = np.mean([l1 == l2 for l1, l2 in zip(labels_a1, labels_a2)])
        
        return {
            "annotators": [annotator1, annotator2],
            "component": component,
            "overlap_count": len(overlap_items),
            "kappa": float(kappa),
            "agreement_rate": float(agreement_rate),
            "interpretation": self._interpret_kappa(kappa)
        }
    
    def fleiss_kappa_multi(self, component: str = None) -> Dict:
        """ì—¬ëŸ¬ annotator ê°„ì˜ Fleiss' Kappa ê³„ì‚°"""
        
        df = self.df.copy()
        if component:
            df = df[df["component"] == component]
        
        # Item Ã— Annotator matrix êµ¬ì„±
        item_annotator = df.pivot_table(
            index="item_id",
            columns="annotator",
            values="label",
            aggfunc="first"
        )
        
        # ëª¨ë“  annotatorê°€ ë¼ë²¨ë§í•œ itemë§Œ ì‚¬ìš©
        complete_items = item_annotator.dropna()
        
        if len(complete_items) == 0:
            return {
                "component": component,
                "n_items": 0,
                "n_annotators": 0,
                "fleiss_kappa": None,
                "error": "No items with complete annotations"
            }
        
        # Labelì„ ìˆ«ìë¡œ ë³€í™˜
        all_labels = df["label"].unique()
        label_to_idx = {label: idx for idx, label in enumerate(sorted(all_labels))}
        
        # Rating matrix êµ¬ì„±: (items Ã— categories)
        n_items = len(complete_items)
        n_categories = len(all_labels)
        rating_matrix = np.zeros((n_items, n_categories))
        
        for i, (item_id, row) in enumerate(complete_items.iterrows()):
            for annotator_label in row.values:
                if pd.notna(annotator_label):
                    rating_matrix[i, label_to_idx[annotator_label]] += 1
        
        # Fleiss' Kappa ê³„ì‚°
        try:
            kappa = fleiss_kappa(rating_matrix, method='fleiss')
        except Exception as e:
            return {
                "component": component,
                "n_items": n_items,
                "n_annotators": len(complete_items.columns),
                "fleiss_kappa": None,
                "error": str(e)
            }
        
        return {
            "component": component,
            "n_items": n_items,
            "n_annotators": len(complete_items.columns),
            "n_categories": n_categories,
            "fleiss_kappa": float(kappa),
            "interpretation": self._interpret_kappa(kappa)
        }
    
    def score_correlation(self, annotator1: str, annotator2: str, 
                         component: str = None) -> Dict:
        """ì—°ì† ì ìˆ˜ (M_score ë“±)ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°"""
        
        df = self.df.copy()
        if component:
            df = df[df["component"] == component]
        
        # 'score' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
        if "score" not in df.columns:
            return {
                "error": "No 'score' column found. Use this for M_score, D_score, F_score."
            }
        
        # Overlap items
        items_a1 = set(df[df["annotator"] == annotator1]["item_id"])
        items_a2 = set(df[df["annotator"] == annotator2]["item_id"])
        overlap_items = items_a1 & items_a2
        
        if len(overlap_items) == 0:
            return {"error": "No overlapping items"}
        
        scores_a1 = []
        scores_a2 = []
        
        for item_id in sorted(overlap_items):
            score1 = df[(df["item_id"] == item_id) & 
                       (df["annotator"] == annotator1)]["score"].values[0]
            score2 = df[(df["item_id"] == item_id) & 
                       (df["annotator"] == annotator2)]["score"].values[0]
            scores_a1.append(float(score1))
            scores_a2.append(float(score2))
        
        # Pearson correlation
        correlation = np.corrcoef(scores_a1, scores_a2)[0, 1]
        
        # MAE (Mean Absolute Error)
        mae = np.mean(np.abs(np.array(scores_a1) - np.array(scores_a2)))
        
        return {
            "annotators": [annotator1, annotator2],
            "component": component,
            "overlap_count": len(overlap_items),
            "pearson_correlation": float(correlation),
            "mae": float(mae),
            "interpretation": self._interpret_correlation(correlation)
        }
    
    @staticmethod
    def _interpret_kappa(kappa: float) -> str:
        """Kappa ê°’ í•´ì„ (Landis & Koch, 1977)"""
        if kappa < 0:
            return "Poor (worse than chance)"
        elif kappa < 0.20:
            return "Slight"
        elif kappa < 0.40:
            return "Fair"
        elif kappa < 0.60:
            return "Moderate"
        elif kappa < 0.80:
            return "Substantial"
        else:
            return "Almost Perfect"
    
    @staticmethod
    def _interpret_correlation(r: float) -> str:
        """ìƒê´€ê³„ìˆ˜ í•´ì„"""
        abs_r = abs(r)
        if abs_r < 0.3:
            return "Weak"
        elif abs_r < 0.7:
            return "Moderate"
        else:
            return "Strong"
    
    def generate_report(self, output_file: str = None) -> Dict:
        """ì „ì²´ IAA ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report = {
            "summary": {},
            "pairwise_kappa": [],
            "fleiss_kappa": [],
            "score_correlations": [],
            "recommendations": []
        }
        
        # ê³ ìœ  annotator, component ì¶”ì¶œ
        annotators = self.df["annotator"].unique()
        components = self.df["component"].unique()
        
        report["summary"] = {
            "n_annotators": len(annotators),
            "n_components": len(components),
            "n_total_annotations": len(self.df),
            "annotators": list(annotators),
            "components": list(components)
        }
        
        # Pairwise Cohen's Kappa
        print("ğŸ“Š ê³„ì‚° ì¤‘: Pairwise Cohen's Kappa...")
        for i, a1 in enumerate(annotators):
            for a2 in annotators[i+1:]:
                for component in components:
                    result = self.cohen_kappa_pairwise(a1, a2, component)
                    if result["kappa"] is not None:
                        report["pairwise_kappa"].append(result)
        
        # Fleiss' Kappa (ê° componentë³„)
        print("ğŸ“Š ê³„ì‚° ì¤‘: Fleiss' Kappa...")
        for component in components:
            result = self.fleiss_kappa_multi(component)
            if result.get("fleiss_kappa") is not None:
                report["fleiss_kappa"].append(result)
        
        # Score correlation (M_score ë“±)
        if "score" in self.df.columns:
            print("ğŸ“Š ê³„ì‚° ì¤‘: Score Correlations...")
            for i, a1 in enumerate(annotators):
                for a2 in annotators[i+1:]:
                    for component in components:
                        result = self.score_correlation(a1, a2, component)
                        if "error" not in result:
                            report["score_correlations"].append(result)
        
        # Recommendations
        avg_kappa = np.mean([r["kappa"] for r in report["pairwise_kappa"]])
        
        if avg_kappa < 0.60:
            report["recommendations"].append({
                "level": "critical",
                "message": f"í‰ê·  Kappa {avg_kappa:.3f} < 0.60. Annotator ì¬êµìœ¡ í•„ìš”.",
                "action": "Guideline ì¬ê²€í† , Golden examples ì¶”ê°€, 1:1 ì½”ì¹­"
            })
        elif avg_kappa < 0.70:
            report["recommendations"].append({
                "level": "warning",
                "message": f"í‰ê·  Kappa {avg_kappa:.3f}ëŠ” acceptableí•˜ì§€ë§Œ ê°œì„  ê°€ëŠ¥.",
                "action": "ë¶ˆì¼ì¹˜ ì‚¬ë¡€ ë¶„ì„, FAQ ì—…ë°ì´íŠ¸"
            })
        else:
            report["recommendations"].append({
                "level": "success",
                "message": f"í‰ê·  Kappa {avg_kappa:.3f} >= 0.70. í’ˆì§ˆ ìš°ìˆ˜!",
                "action": "í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìœ ì§€"
            })
        
        # ì¶œë ¥
        if output_file:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
        return report
    
    def print_summary(self, report: Dict):
        """ë¦¬í¬íŠ¸ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š Inter-Annotator Agreement Report")
        print("="*60)
        
        summary = report["summary"]
        print(f"\nğŸ“Œ ê¸°ë³¸ ì •ë³´:")
        print(f"  - Annotators: {summary['n_annotators']}ëª…")
        print(f"  - Components: {', '.join(summary['components'])}")
        print(f"  - Total Annotations: {summary['n_total_annotations']}ê°œ")
        
        if report["pairwise_kappa"]:
            print(f"\nğŸ“Š Pairwise Cohen's Kappa:")
            for result in report["pairwise_kappa"]:
                print(f"  - {result['annotators'][0]} vs {result['annotators'][1]} "
                      f"({result['component']}): Îº = {result['kappa']:.3f} "
                      f"({result['interpretation']})")
        
        if report["fleiss_kappa"]:
            print(f"\nğŸ“Š Fleiss' Kappa (Multi-rater):")
            for result in report["fleiss_kappa"]:
                print(f"  - {result['component']}: Îº = {result['fleiss_kappa']:.3f} "
                      f"({result['interpretation']})")
        
        if report["score_correlations"]:
            print(f"\nğŸ“Š Score Correlations:")
            for result in report["score_correlations"]:
                print(f"  - {result['annotators'][0]} vs {result['annotators'][1]} "
                      f"({result['component']}): r = {result['pearson_correlation']:.3f}, "
                      f"MAE = {result['mae']:.3f}")
        
        print(f"\nğŸ’¡ Recommendations:")
        for rec in report["recommendations"]:
            emoji = {"critical": "ğŸš¨", "warning": "âš ï¸", "success": "âœ…"}[rec["level"]]
            print(f"  {emoji} {rec['message']}")
            print(f"     â†’ {rec['action']}")
        
        print("\n" + "="*60)

def load_annotations_from_label_studio(json_file: str) -> List[Dict]:
    """Label Studio export JSONì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    with open(json_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    annotations = []
    
    for task in data:
        item_id = task.get("data", {}).get("item_id", task["id"])
        
        for annotation in task.get("annotations", []):
            annotator = annotation["completed_by"]
            
            # Label ì¶”ì¶œ (taskì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            result = annotation["result"][0]
            label = result["value"].get("choices", [None])[0]
            
            # Score ì¶”ì¶œ (ìˆë‹¤ë©´)
            score = result["value"].get("rating")
            
            annotations.append({
                "item_id": str(item_id),
                "annotator": f"annotator_{annotator}",
                "label": label,
                "score": score,
                "component": task.get("data", {}).get("component", "unknown")
            })
    
    return annotations

def main():
    parser = argparse.ArgumentParser(description="IAA ê³„ì‚°")
    parser.add_argument("--input", required=True, help="Annotations JSON íŒŒì¼")
    parser.add_argument("--output", default="iaa_report.json", help="ì¶œë ¥ íŒŒì¼")
    parser.add_argument("--format", choices=["standard", "label_studio"], 
                       default="label_studio",
                       help="ì…ë ¥ íŒŒì¼ í˜•ì‹")
    
    args = parser.parse_args()
    
    # Load annotations
    if args.format == "label_studio":
        annotations = load_annotations_from_label_studio(args.input)
    else:
        with open(args.input, "r", encoding="utf-8") as f:
            annotations = json.load(f)
    
    print(f"ğŸ“¥ {len(annotations)}ê°œ annotation ë¡œë“œë¨")
    
    # Calculate IAA
    calculator = IAACalculator(annotations)
    report = calculator.generate_report(args.output)
    
    # Print summary
    calculator.print_summary(report)

if __name__ == "__main__":
    main()
```

## 1ï¸âƒ£ `schemas/schema_M_premise_purification.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/schema_M_premise_purification.json",
  "title": "IRF-DF-Mini M_premise_purification Sample",
  "type": "object",
  "additionalProperties": false,
  "required": ["meta", "task", "input", "label"],
  "properties": {
    "meta": {
      "type": "object",
      "additionalProperties": false,
      "required": ["id", "split"],
      "properties": {
        "id": {
          "type": "string",
          "pattern": "^M_\\d{6}$"
        },
        "version": {
          "type": "string"
        },
        "created_at": {
          "type": "string"
        },
        "source": {
          "type": "object",
          "additionalProperties": false,
          "required": ["type"],
          "properties": {
            "type": {
              "type": "string",
              "enum": [
                "github_pr",
                "textbook",
                "synthetic",
                "proof_log",
                "bug_report",
                "other"
              ]
            },
            "url": {
              "type": "string"
            }
          }
        },
        "annotators": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "primary": { "type": "string" },
            "reviewers": {
              "type": "array",
              "items": { "type": "string" }
            }
          }
        },
        "split": {
          "type": "string",
          "enum": ["train", "dev", "test", "golden"]
        }
      }
    },
    "task": {
      "type": "string",
      "const": "M_premise_purification"
    },
    "input": {
      "type": "object",
      "additionalProperties": false,
      "required": ["question", "context", "model_response"],
      "properties": {
        "question": { "type": "string" },
        "context": { "type": "string" },
        "model_response": { "type": "string" }
      }
    },
    "label": {
      "type": "object",
      "additionalProperties": false,
      "required": ["premise_list", "suspect_flags", "M_score", "M_rationale"],
      "properties": {
        "premise_list": {
          "type": "array",
          "items": {
            "type": "object",
            "additionalProperties": false,
            "required": ["text", "source"],
            "properties": {
              "text": { "type": "string" },
              "source": {
                "type": "string",
                "enum": ["explicit", "implicit"]
              }
            }
          }
        },
        "suspect_flags": {
          "type": "array",
          "items": {
            "type": "object",
            "additionalProperties": false,
            "required": ["premise_index", "reason"],
            "properties": {
              "premise_index": {
                "type": "integer",
                "minimum": 0
              },
              "reason": { "type": "string" }
            }
          }
        },
        "missing_premises": {
          "type": "array",
          "items": { "type": "string" }
        },
        "M_score": {
          "type": "number",
          "minimum": 0.0,
          "maximum": 1.0
        },
        "M_rationale": {
          "type": "string"
        }
      }
    }
  }
}

```

---

## 2ï¸âƒ£ `schemas/schema_D_deduction_pairwise.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/schema_D_deduction_pairwise.json",
  "title": "IRF-DF-Mini D_deduction_pairwise Sample",
  "type": "object",
  "additionalProperties": false,
  "required": ["meta", "task", "input", "label"],
  "properties": {
    "meta": {
      "type": "object",
      "additionalProperties": false,
      "required": ["id", "split"],
      "properties": {
        "id": {
          "type": "string",
          "pattern": "^D_\\d{6}$"
        },
        "version": {
          "type": "string"
        },
        "created_at": {
          "type": "string"
        },
        "source": {
          "type": "object",
          "additionalProperties": false,
          "required": ["type"],
          "properties": {
            "type": {
              "type": "string",
              "enum": [
                "github_pr",
                "textbook",
                "synthetic",
                "proof_log",
                "bug_report",
                "other"
              ]
            },
            "url": {
              "type": "string"
            }
          }
        },
        "annotators": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "primary": { "type": "string" },
            "reviewers": {
              "type": "array",
              "items": { "type": "string" }
            }
          }
        },
        "split": {
          "type": "string",
          "enum": ["train", "dev", "test", "golden"]
        }
      }
    },
    "task": {
      "type": "string",
      "const": "D_deduction_pairwise"
    },
    "input": {
      "type": "object",
      "additionalProperties": false,
      "required": ["premises", "hypothesis", "response_A", "response_B"],
      "properties": {
        "premises": { "type": "string" },
        "hypothesis": { "type": "string" },
        "response_A": { "type": "string" },
        "response_B": { "type": "string" }
      }
    },
    "label": {
      "type": "object",
      "additionalProperties": false,
      "required": ["preference", "D_rationale"],
      "properties": {
        "preference": {
          "type": "string",
          "enum": ["A_better", "B_better", "tie"]
        },
        "D_rationale": {
          "type": "string"
        }
      }
    }
  }
}

```

---

## 3ï¸âƒ£ `schemas/schema_F_falsification_pairwise.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/schema_F_falsification_pairwise.json",
  "title": "IRF-DF-Mini F_falsification_pairwise Sample",
  "type": "object",
  "additionalProperties": false,
  "required": ["meta", "task", "input", "label"],
  "properties": {
    "meta": {
      "type": "object",
      "additionalProperties": false,
      "required": ["id", "split"],
      "properties": {
        "id": {
          "type": "string",
          "pattern": "^F_\\d{6}$"
        },
        "version": {
          "type": "string"
        },
        "created_at": {
          "type": "string"
        },
        "source": {
          "type": "object",
          "additionalProperties": false,
          "required": ["type"],
          "properties": {
            "type": {
              "type": "string",
              "enum": [
                "github_pr",
                "textbook",
                "synthetic",
                "proof_log",
                "bug_report",
                "other"
              ]
            },
            "url": {
              "type": "string"
            }
          }
        },
        "annotators": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "primary": { "type": "string" },
            "reviewers": {
              "type": "array",
              "items": { "type": "string" }
            }
          }
        },
        "split": {
          "type": "string",
          "enum": ["train", "dev", "test", "golden"]
        }
      }
    },
    "task": {
      "type": "string",
      "const": "F_falsification_pairwise"
    },
    "input": {
      "type": "object",
      "additionalProperties": false,
      "required": ["hypothesis", "evidence", "response_A", "response_B"],
      "properties": {
        "hypothesis": { "type": "string" },
        "evidence": { "type": "string" },
        "response_A": { "type": "string" },
        "response_B": { "type": "string" }
      }
    },
    "label": {
      "type": "object",
      "additionalProperties": false,
      "required": ["preference", "F_rationale"],
      "properties": {
        "preference": {
          "type": "string",
          "enum": ["A_better", "B_better", "tie"]
        },
        "F_rationale": {
          "type": "string"
        },
        "proposed_test_cases": {
          "type": "array",
          "items": { "type": "string" }
        },
        "falsification_verdict": {
          "type": "string",
          "enum": [
            "hypothesis_rejected",
            "weakened",
            "inconclusive"
          ]
        },
        "revised_hypothesis": {
          "type": ["string", "null"]
        }
      }
    }
  }
}

```

---

## 4ï¸âƒ£ `scripts/validate_annotation.py` (í•œ íŒŒì¼ ì™„ì „ì²´)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:

- `schemas/` í´ë”ì—ì„œ ìœ„ 3ê°œ ìŠ¤í‚¤ë§ˆë¥¼ ì½ê³ 
- ì¸ìë¡œ ë°›ì€ ê²½ë¡œ(ë””ë ‰í† ë¦¬ or íŒŒì¼) ì•„ë˜ì˜ ëª¨ë“  `.json`ì„ ê²€ì‚¬í•´ì„œ
- ì–´ë–¤ íŒŒì¼ì´ ì–´ë–¤ ì—ëŸ¬ë¡œ ê¹¨ì§€ëŠ”ì§€ ì¶œë ¥í•´ì¤€ë‹¤.

```python
#!/usr/bin/env python
"""
validate_annotation.py

IRF-DF-Mini annotation JSON íŒŒì¼ë“¤ì„ JSON Schemaë¡œ ê²€ì¦í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

ì‚¬ìš© ì˜ˆì‹œ:
    # ë¦¬í¬ ë£¨íŠ¸ì—ì„œ
    python scripts/validate_annotation.py data/df_mini

    # íŠ¹ì • íŒŒì¼ë§Œ
    python scripts/validate_annotation.py data/df_mini/M/M_000001.json
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any

try:
    import jsonschema
    from jsonschema import Draft7Validator
except ImportError:
    print(
        "[ERROR] jsonschema íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
        "pip install jsonschema\n"
    )
    sys.exit(1)

ROOT_DIR = Path(__file__).resolve().parents[1]
SCHEMA_DIR = ROOT_DIR / "schemas"

def load_schema(name: str) -> Dict[str, Any]:
    path = SCHEMA_DIR / name
    if not path.exists():
        raise FileNotFoundError(f"Schema file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

SCHEMAS = {
    "M_premise_purification": load_schema("schema_M_premise_purification.json"),
    "D_deduction_pairwise": load_schema("schema_D_deduction_pairwise.json"),
    "F_falsification_pairwise": load_schema("schema_F_falsification_pairwise.json"),
}

VALIDATORS = {
    task: Draft7Validator(schema) for task, schema in SCHEMAS.items()
}

def guess_task_from_id(meta: Dict[str, Any]) -> str:
    """
    meta.id prefixë¡œ taskë¥¼ ì¶”ì¸¡í•˜ëŠ” ë³´ì¡° í•¨ìˆ˜.
    - M_****** â†’ M_premise_purification
    - D_****** â†’ D_deduction_pairwise
    - F_****** â†’ F_falsification_pairwise
    """
    id_ = meta.get("id", "")
    if id_.startswith("M_"):
        return "M_premise_purification"
    if id_.startswith("D_"):
        return "D_deduction_pairwise"
    if id_.startswith("F_"):
        return "F_falsification_pairwise"
    return ""

def validate_instance(instance: Dict[str, Any], path: Path) -> bool:
    """
    ë‹¨ì¼ JSON ê°ì²´(instance)ì— ëŒ€í•´:
      1) taskì— ë§ëŠ” schema ì„ íƒ
      2) jsonschemaë¡œ validate
      3) ì¶”ê°€ë¡œ meta.id prefixì™€ task ì¼ê´€ì„± ê²€ì‚¬

    ë°˜í™˜ê°’:
      True  = ìœ íš¨
      False = ìœ íš¨í•˜ì§€ ì•ŠìŒ
    """
    errors = []

    task = instance.get("task")
    meta = instance.get("meta", {})

    if not task:
        errors.append("Missing 'task' field.")
    elif task not in VALIDATORS:
        errors.append(f"Unknown task '{task}'. "
                      f"Expected one of {list(VALIDATORS.keys())}.")

    # meta.id prefix check
    if isinstance(meta, dict) and "id" in meta:
        guessed_task = guess_task_from_id(meta)
        if guessed_task and task and guessed_task != task:
            errors.append(
                f"meta.id '{meta['id']}' implies task '{guessed_task}', "
                f"but 'task' field is '{task}'."
            )

    if errors:
        print(f"[INVALID] {path}")
        for e in errors:
            print(f"  - {e}")
        return False

    # jsonschema validate
    validator = VALIDATORS[task]
    validation_errors = sorted(validator.iter_errors(instance), key=lambda e: e.path)

    if validation_errors:
        print(f"[INVALID] {path}")
        for err in validation_errors:
            # ì—ëŸ¬ ê²½ë¡œë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ í‘œì‹œ
            loc = " -> ".join(str(x) for x in err.path) or "(root)"
            print(f"  - At {loc}: {err.message}")
        return False

    # OK
    return True

def iter_json_files(target: Path):
    """
    targetì´:
      - ë””ë ‰í† ë¦¬ë©´: í•˜ìœ„ ëª¨ë“  .json íŒŒì¼
      - íŒŒì¼ì´ë©´: ê·¸ íŒŒì¼ í•˜ë‚˜
    ë¥¼ yield
    """
    if target.is_dir():
        yield from target.rglob("*.json")
    else:
        if target.suffix.lower() == ".json":
            yield target

def main(argv=None):
    argv = argv or sys.argv[1:]

    if not argv:
        print(
            "Usage:\n"
            "  python scripts/validate_annotation.py <path-to-json-or-dir>\n\n"
            "ì˜ˆì‹œ:\n"
            "  python scripts/validate_annotation.py data/df_mini\n"
            "  python scripts/validate_annotation.py data/df_mini/M/M_000001.json\n"
        )
        return 1

    target = Path(argv[0]).resolve()
    if not target.exists():
        print(f"[ERROR] Target not found: {target}")
        return 1

    total = 0
    ok = 0

    for path in iter_json_files(target):
        total += 1
        try:
            with path.open("r", encoding="utf-8") as f:
                instance = json.load(f)
        except Exception as e:
            print(f"[INVALID] {path}")
            print(f"  - JSON parse error: {e}")
            continue

        if validate_instance(instance, path):
            ok += 1

    print()
    print(f"Validation finished. {ok}/{total} files valid.")

    return 0 if ok == total else 2

if __name__ == "__main__":
    raise SystemExit(main())

```

---

### ğŸ“ ì‚¬ìš© ìš”ì•½

1. ìœ„ JSON Schema 3ê°œë¥¼ `schemas/`ì— ì €ì¥:
    - `schemas/schema_M_premise_purification.json`
    - `schemas/schema_D_deduction_pairwise.json`
    - `schemas/schema_F_falsification_pairwise.json`
2. Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ `scripts/validate_annotation.py`ë¡œ ì €ì¥
3. ë¦¬í¬ ë£¨íŠ¸ì—ì„œ:

```bash
pip install jsonschema

python scripts/validate_annotation.py data/df_mini
# ë˜ëŠ”
python scripts/validate_annotation.py data/df_mini/M/M_000001.json

```

ì´ì œë¶€í„°ëŠ” annotatorê°€ ë§Œë“  JSONì´ **ìŠ¤í‚¤ë§ˆ/ID/Task ì¼ê´€ì„±**ê¹Œì§€ ìë™ ê²€ì¦ëœë‹¤.

ì¶”ê°€ë¡œ:

- ë‹¤ë¥¸ í•„ë“œ(ì˜ˆ: `falsification_verdict` ê°•ì œ, `proposed_test_cases` í•„ìˆ˜í™” ë“±)ë¥¼ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´
    
    â†’ í•´ë‹¹ Schema JSONë§Œ ìˆ˜ì •í•˜ë©´ íŒŒì´í”„ë¼ì¸ ì „ì²´ê°€ ë°”ë¡œ ê·¸ ê·œì¹™ì„ ë”°ë¥¸ë‹¤.
    

## **ê³¨ë“  ì˜ˆì œ ìŠ¤í‚¤ë§ˆ + validator** ì„¸íŠ¸

## `schemas/schema_golden_M_premise_purification.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/schema_golden_M_premise_purification.json",
  "title": "IRF-DF-Mini Golden Example - M (Premise Purification)",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "golden_id",
    "component",
    "task_type",
    "input",
    "model_response",
    "golden_label"
  ],
  "properties": {
    "golden_id": {
      "type": "string",
      "pattern": "^M_\\d{6}$"
    },
    "component": {
      "type": "string",
      "const": "M"
    },
    "task_type": {
      "type": "string",
      "const": "premise_purification"
    },
    "input": {
      "type": "object",
      "additionalProperties": false,
      "required": ["question", "context"],
      "properties": {
        "question": { "type": "string" },
        "context": { "type": "string" }
      }
    },
    "model_response": {
      "type": "string"
    },
    "golden_label": {
      "type": "object",
      "additionalProperties": false,
      "required": [
        "premise_list",
        "suspect_flags",
        "M_score",
        "M_rationale"
      ],
      "properties": {
        "premise_list": {
          "type": "array",
          "minItems": 1,
          "items": {
            "type": "object",
            "additionalProperties": false,
            "required": ["text", "source"],
            "properties": {
              "text": { "type": "string" },
              "source": {
                "type": "string",
                "enum": ["explicit", "implicit"]
              }
            }
          }
        },
        "suspect_flags": {
          "type": "array",
          "items": {
            "type": "object",
            "additionalProperties": false,
            "required": ["premise_index", "reason"],
            "properties": {
              "premise_index": {
                "type": "integer",
                "minimum": 0
              },
              "reason": { "type": "string" }
            }
          }
        },
        "missing_premises": {
          "type": "array",
          "items": { "type": "string" }
        },
        "M_score": {
          "type": "number",
          "minimum": 0.0,
          "maximum": 1.0
        },
        "M_rationale": {
          "type": "string"
        }
      }
    },
    "annotation_notes": {
      "type": "string"
    }
  }
}

```

---

## 2ï¸âƒ£ `schemas/schema_golden_D_deduction_pairwise.json`

(ê³¨ë“  D ì˜ˆì œ í¬ë§·: `golden_id`, `component`, `task_type`, `input`ì— premises/hypothesis/response_A/response_B, `golden_label`ì— preference + D_rationale.)

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/schema_golden_D_deduction_pairwise.json",
  "title": "IRF-DF-Mini Golden Example - D (Deduction Pairwise)",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "golden_id",
    "component",
    "task_type",
    "input",
    "golden_label"
  ],
  "properties": {
    "golden_id": {
      "type": "string",
      "pattern": "^D_\\d{6}$"
    },
    "component": {
      "type": "string",
      "const": "D"
    },
    "task_type": {
      "type": "string",
      "const": "deduction_pairwise"
    },
    "input": {
      "type": "object",
      "additionalProperties": false,
      "required": [
        "premises",
        "hypothesis",
        "response_A",
        "response_B"
      ],
      "properties": {
        "premises": { "type": "string" },
        "hypothesis": { "type": "string" },
        "response_A": { "type": "string" },
        "response_B": { "type": "string" }
      }
    },
    "golden_label": {
      "type": "object",
      "additionalProperties": false,
      "required": ["preference", "D_rationale"],
      "properties": {
        "preference": {
          "type": "string",
          "enum": ["A_better", "B_better", "tie"]
        },
        "D_rationale": {
          "type": "string"
        }
      }
    },
    "annotation_notes": {
      "type": "string"
    }
  }
}

```

---

## 3ï¸âƒ£ `schemas/schema_golden_F_falsification_pairwise.json`

(F ê³¨ë“  ì˜ˆì œ: hypothesis/evidence/response_A/B, golden_labelì— preference/F_rationale + optional í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤/íŒì •/ìˆ˜ì •ê°€ì„¤)

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://example.com/schema_golden_F_falsification_pairwise.json",
  "title": "IRF-DF-Mini Golden Example - F (Falsification Pairwise)",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "golden_id",
    "component",
    "task_type",
    "input",
    "golden_label"
  ],
  "properties": {
    "golden_id": {
      "type": "string",
      "pattern": "^F_\\d{6}$"
    },
    "component": {
      "type": "string",
      "const": "F"
    },
    "task_type": {
      "type": "string",
      "const": "falsification_pairwise"
    },
    "input": {
      "type": "object",
      "additionalProperties": false,
      "required": [
        "hypothesis",
        "evidence",
        "response_A",
        "response_B"
      ],
      "properties": {
        "hypothesis": { "type": "string" },
        "evidence": { "type": "string" },
        "response_A": { "type": "string" },
        "response_B": { "type": "string" }
      }
    },
    "golden_label": {
      "type": "object",
      "additionalProperties": false,
      "required": ["preference", "F_rationale"],
      "properties": {
        "preference": {
          "type": "string",
          "enum": ["A_better", "B_better", "tie"]
        },
        "F_rationale": {
          "type": "string"
        },
        "proposed_test_cases": {
          "type": "array",
          "items": { "type": "string" }
        },
        "falsification_verdict": {
          "type": "string",
          "enum": [
            "hypothesis_rejected",
            "weakened",
            "inconclusive"
          ]
        },
        "revised_hypothesis": {
          "type": ["string", "null"]
        }
      }
    },
    "annotation_notes": {
      "type": "string"
    }
  }
}

```

---

## 4ï¸âƒ£ `scripts/validate_annotation.py` (ê³¨ë“  ì˜ˆì œ ê²€ì¦ìš© ì™„ì „ì²´)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:

- `schemas/` í´ë”ì—ì„œ ìœ„ 3ê°œ ìŠ¤í‚¤ë§ˆë¥¼ ì½ê³ 
- ì¸ìë¡œ ë°›ì€ ê²½ë¡œ(íŒŒì¼ or ë””ë ‰í† ë¦¬) ì•„ë˜ `.json` ì „ë¶€ ì½ì–´ì„œ
- `component` + `task_type` ì¡°í•©ì— ë§ëŠ” ìŠ¤í‚¤ë§ˆë¡œ validate
    
    (í˜•ì‹/í•„ë“œ ëˆ„ë½/íƒ€ì… ì—ëŸ¬/ID íŒ¨í„´ ë“± ë‹¤ ì¡ì•„ì¤Œ)
    

```python
#!/usr/bin/env python3
"""
IRF-DF-Mini Golden Example Validator

- golden_examples/M/*.json
- golden_examples/D/*.json
- golden_examples/F/*.json

ì´ íŒŒì¼ë“¤ì´ schema_golden_*.jsonì— ë§ê²Œ ì˜ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•œë‹¤.

Usage:
    # ë¦¬í¬ ë£¨íŠ¸ì—ì„œ
    python scripts/validate_annotation.py golden_examples

    # íŠ¹ì • íŒŒì¼ë§Œ
    python scripts/validate_annotation.py golden_examples/M/M_000001_browser_premise.json
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any, Optional

try:
    import jsonschema
    from jsonschema import Draft7Validator
except ImportError:
    print(
        "[ERROR] jsonschema íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
        "ì„¤ì¹˜:\n"
        "    pip install jsonschema\n"
    )
    sys.exit(1)

ROOT_DIR = Path(__file__).resolve().parents[1]
SCHEMA_DIR = ROOT_DIR / "schemas"

def load_schema(name: str) -> Dict[str, Any]:
    """schemas/ ì•„ë˜ JSON Schema ë¡œë“œ"""
    path = SCHEMA_DIR / name
    if not path.exists():
        raise FileNotFoundError(f"Schema file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

# ìŠ¤í‚¤ë§ˆ ë¡œë“œ
SCHEMAS = {
    ("M", "premise_purification"): load_schema("schema_golden_M_premise_purification.json"),
    ("D", "deduction_pairwise"): load_schema("schema_golden_D_deduction_pairwise.json"),
    ("F", "falsification_pairwise"): load_schema("schema_golden_F_falsification_pairwise.json"),
}

VALIDATORS = {
    key: Draft7Validator(schema) for key, schema in SCHEMAS.items()
}

def pick_validator(component: str, task_type: str) -> Optional[Draft7Validator]:
    """component + task_type ì¡°í•©ìœ¼ë¡œ validator ì„ íƒ"""
    return VALIDATORS.get((component, task_type))

def validate_instance(instance: Dict[str, Any], path: Path) -> bool:
    """
    ë‹¨ì¼ JSON ê³¨ë“  ì˜ˆì œë¥¼ ê²€ì¦:
      1) component / task_type ì¡´ì¬ í™•ì¸
      2) ì˜¬ë°”ë¥¸ ìŠ¤í‚¤ë§ˆ ì„ íƒ
      3) jsonschemaë¡œ validate

    ë°˜í™˜:
      True  = ìœ íš¨
      False = ìœ íš¨í•˜ì§€ ì•ŠìŒ
    """
    errors = []

    component = instance.get("component")
    task_type = instance.get("task_type")
    golden_id = instance.get("golden_id")

    if not golden_id:
        errors.append("Missing 'golden_id' field.")

    if not component:
        errors.append("Missing 'component' field.")
    if not task_type:
        errors.append("Missing 'task_type' field.")

    validator = None
    if component and task_type:
        validator = pick_validator(component, task_type)
        if validator is None:
            valid_keys = ", ".join(f"{c}/{t}" for (c, t) in VALIDATORS.keys())
            errors.append(
                f"Unsupported component/task_type combination: '{component}/{task_type}'. "
                f"ì§€ì›ë˜ëŠ” ì¡°í•©: {valid_keys}"
            )

    if errors:
        print(f"[INVALID] {path}")
        for e in errors:
            print(f"  - {e}")
        return False

    # jsonschemaë¡œ ì‹¤ì œ í•„ë“œ/íƒ€ì… ê²€ì¦
    validation_errors = sorted(validator.iter_errors(instance), key=lambda e: e.path)

    if validation_errors:
        print(f"[INVALID] {path}")
        for err in validation_errors:
            loc = " -> ".join(str(x) for x in err.path) or "(root)"
            print(f"  - At {loc}: {err.message}")
        return False

    # OK
    return True

def iter_json_files(target: Path):
    """
    targetì´:
      - ë””ë ‰í† ë¦¬ë©´: í•˜ìœ„ ëª¨ë“  .json íŒŒì¼
      - íŒŒì¼ì´ë©´: ê·¸ íŒŒì¼ í•˜ë‚˜
    ë¥¼ yield
    """
    if target.is_dir():
        yield from target.rglob("*.json")
    else:
        if target.suffix.lower() == ".json":
            yield target

def main(argv=None):
    argv = argv or sys.argv[1:]

    if not argv:
        print(
            "Usage:\n"
            "  python scripts/validate_annotation.py <path-to-json-or-dir>\n\n"
            "ì˜ˆì‹œ:\n"
            "  python scripts/validate_annotation.py golden_examples\n"
            "  python scripts/validate_annotation.py golden_examples/M/M_000001_browser_premise.json\n"
        )
        return 1

    target = Path(argv[0]).resolve()
    if not target.exists():
        print(f"[ERROR] Target not found: {target}")
        return 1

    total = 0
    ok = 0

    for path in iter_json_files(target):
        total += 1
        try:
            with path.open("r", encoding="utf-8") as f:
                instance = json.load(f)
        except Exception as e:
            print(f"[INVALID] {path}")
            print(f"  - JSON parse error: {e}")
            continue

        if validate_instance(instance, path):
            ok += 1
            print(f"[OK]      {path}")

    print()
    print(f"Validation finished. {ok}/{total} files valid.")

    return 0 if ok == total else 2

if __name__ == "__main__":
    raise SystemExit(main())

```

---

### ğŸ”Œ ë°”ë¡œ ì“°ëŠ” ë°©ë²• ì •ë¦¬

í”„ë¡œì íŠ¸ ë£¨íŠ¸ êµ¬ì¡° ì˜ˆ:

```
irf-df-mini-toolkit/
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ schema_golden_M_premise_purification.json
â”‚   â”œâ”€â”€ schema_golden_D_deduction_pairwise.json
â”‚   â””â”€â”€ schema_golden_F_falsification_pairwise.json
â”œâ”€â”€ golden_examples/
â”‚   â”œâ”€â”€ M/
â”‚   â”‚   â””â”€â”€ M_000001_browser_premise.json
â”‚   â”œâ”€â”€ D/
â”‚   â””â”€â”€ F/
â””â”€â”€ scripts/
    â””â”€â”€ validate_annotation.py

```

ì„¤ì¹˜ + ì‹¤í–‰:

```bash
pip install jsonschema

# ë¦¬í¬ ë£¨íŠ¸ì—ì„œ
python scripts/validate_annotation.py golden_examples
# ë˜ëŠ”
python scripts/validate_annotation.py golden_examples/M/M_000001_browser_premise.json

```

ì´ì œ **ê³¨ë“  12ê°œ + ì´í›„ ì¶”ê°€ ê³¨ë“ ë“¤**ì„ ì „ë¶€ ì´ ìŠ¤í‚¤ë§ˆë¡œ ìë™ ê²€ì‚¬í•  ìˆ˜ ìˆìŒ.

## **ê³¨ë“  â†’ í•™ìŠµ í¬ë§·**, ê·¸ë¦¬ê³  **Label Studio â†’ 2ì°¨ ê²€ì¦**ê¹Œì§€ â€œì™„ì „í•œ íŒŒì´í”„ë¼ì¸â€

## 1ï¸âƒ£ `convert_golden_to_train.py`

ì—­í• :

- `golden_examples/M|D|F/*.json` (ì´ë¯¸ ìš°ë¦¬ê°€ ì •í•œ golden schema ì¤€ìˆ˜) â†’
    
    `data/train/train_M.jsonl`, `train_D.jsonl`, `train_F.jsonl` ë¡œ ë³€í™˜
    
- **M**: supervised í•™ìŠµìš© (`instruction` + `target` + êµ¬ì¡°í™” label)
- **D/F**: pairwise preference í•™ìŠµìš© (ë­í‚¹/ì„ í˜¸ ëª¨ë¸, ë˜ëŠ” LLM instruction-finetune)

```python
#!/usr/bin/env python3
"""
convert_golden_to_train.py

ê³¨ë“  ì˜ˆì œ(M/D/F)ë¥¼ IRF-DF-Mini ë‚´ë¶€ í•™ìŠµ í¬ë§·(JSONL)ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

ì…ë ¥ êµ¬ì¡°(ë¦¬í¬ ë£¨íŠ¸ ê¸°ì¤€):
  golden_examples/
    M/*.json  (M_000001_*.json ...)
    D/*.json
    F/*.json
  schemas/
    schema_golden_M_premise_purification.json
    schema_golden_D_deduction_pairwise.json
    schema_golden_F_falsification_pairwise.json

ì¶œë ¥:
  data/train/train_M.jsonl
  data/train/train_D.jsonl
  data/train/train_F.jsonl

ìš”êµ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬:
  pip install jsonschema
"""

import json
from pathlib import Path
from typing import Dict, Any, Tuple

import jsonschema
from jsonschema import Draft7Validator

ROOT_DIR = Path(__file__).resolve().parents[1]
GOLDEN_DIR = ROOT_DIR / "golden_examples"
SCHEMA_DIR = ROOT_DIR / "schemas"
TRAIN_DIR = ROOT_DIR / "data" / "train"

def load_schema(name: str) -> Dict[str, Any]:
    path = SCHEMA_DIR / name
    if not path.exists():
        raise FileNotFoundError(f"Schema file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

# ìŠ¤í‚¤ë§ˆ ë¡œë“œ (ì´ì „ì— ì •ì˜í•œ ê²ƒê³¼ ë™ì¼í•œ íŒŒì¼ëª… ì‚¬ìš©)
SCHEMA_M = load_schema("schema_golden_M_premise_purification.json")
SCHEMA_D = load_schema("schema_golden_D_deduction_pairwise.json")
SCHEMA_F = load_schema("schema_golden_F_falsification_pairwise.json")

VALIDATORS = {
    ("M", "premise_purification"): Draft7Validator(SCHEMA_M),
    ("D", "deduction_pairwise"): Draft7Validator(SCHEMA_D),
    ("F", "falsification_pairwise"): Draft7Validator(SCHEMA_F),
}

def validate_golden(instance: Dict[str, Any]) -> Tuple[bool, str]:
    """ê³¨ë“  ì˜ˆì œê°€ ìŠ¤í‚¤ë§ˆë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì¦."""
    comp = instance.get("component")
    task_type = instance.get("task_type")
    key = (comp, task_type)

    if key not in VALIDATORS:
        return False, f"Unsupported component/task_type: {comp}/{task_type}"

    validator = VALIDATORS[key]
    errors = sorted(validator.iter_errors(instance), key=lambda e: e.path)
    if errors:
        msg_lines = []
        for err in errors:
            loc = " -> ".join(str(x) for x in err.path) or "(root)"
            msg_lines.append(f"At {loc}: {err.message}")
        return False, "; ".join(msg_lines)

    return True, ""

def convert_M(instance: Dict[str, Any]) -> Dict[str, Any]:
    """
    M ê³¨ë“  ì˜ˆì œ â†’ í•™ìŠµ í¬ë§·

    ë‚´ë¶€ í¬ë§· ì˜ˆ:
    {
      "id": "M_000001",
      "component": "M",
      "task_type": "premise_purification",
      "input": {
        "question": "...",
        "context": "...",
        "model_response": "..."
      },
      "labels": {... golden_label ...},
      "training_example": {
        "instruction": "...",
        "target": "..."  # JSON string ë˜ëŠ” ìì—°ì–´ ìš”ì•½
      }
    }
    """
    golden_id = instance["golden_id"]
    inp = instance["input"]
    label = instance["golden_label"]

    instruction = (
        "You are the M (Premise Purification) component of IRF-DF-Mini.\n"
        "Given the question, context, and model_response, list:\n"
        "1) explicit premises\n"
        "2) implicit premises\n"
        "3) which premises are suspect and why\n\n"
        "Return a JSON object with keys: premise_list, suspect_flags, M_score.\n\n"
        f"Question: {inp['question']}\n\n"
        f"Context: {inp['context']}\n\n"
        f"Model response:\n{instance['model_response']}\n"
    )

    # targetì€ golden_labelì„ JSON ë¬¸ìì—´ë¡œ ë„£ì–´ì¤€ë‹¤
    target = json.dumps(
        {
            "premise_list": label["premise_list"],
            "suspect_flags": label["suspect_flags"],
            "M_score": label["M_score"],
        },
        ensure_ascii=False,
        indent=2,
    )

    return {
        "id": golden_id,
        "component": "M",
        "task_type": "premise_purification",
        "input": {
            "question": inp["question"],
            "context": inp["context"],
            "model_response": instance["model_response"],
        },
        "labels": label,
        "training_example": {
            "instruction": instruction,
            "target": target,
        },
    }

def convert_D(instance: Dict[str, Any]) -> Dict[str, Any]:
    """
    D ê³¨ë“  ì˜ˆì œ â†’ pairwise í•™ìŠµ í¬ë§·

    ë‚´ë¶€ í¬ë§· ì˜ˆ:
    {
      "id": "D_000001",
      "component": "D",
      "task_type": "deduction_pairwise",
      "prompt": {...},
      "response_A": "...",
      "response_B": "...",
      "preference": "A_better",
      "rationale": "..."
    }
    """
    golden_id = instance["golden_id"]
    inp = instance["input"]
    label = instance["golden_label"]

    return {
        "id": golden_id,
        "component": "D",
        "task_type": "deduction_pairwise",
        "prompt": {
            "premises": inp["premises"],
            "hypothesis": inp["hypothesis"],
        },
        "response_A": inp["response_A"],
        "response_B": inp["response_B"],
        "preference": label["preference"],
        "rationale": label["D_rationale"],
    }

def convert_F(instance: Dict[str, Any]) -> Dict[str, Any]:
    """
    F ê³¨ë“  ì˜ˆì œ â†’ pairwise í•™ìŠµ í¬ë§·

    ë‚´ë¶€ í¬ë§· ì˜ˆ:
    {
      "id": "F_000001",
      "component": "F",
      "task_type": "falsification_pairwise",
      "prompt": {...},
      "response_A": "...",
      "response_B": "...",
      "preference": "B_better",
      "rationale": "...",
      "extra": {...}
    }
    """
    golden_id = instance["golden_id"]
    inp = instance["input"]
    label = instance["golden_label"]

    # optional fields ì•ˆì „í•˜ê²Œ ë¹¼ê¸°
    extra = {
        "proposed_test_cases": label.get("proposed_test_cases", []),
        "falsification_verdict": label.get("falsification_verdict"),
        "revised_hypothesis": label.get("revised_hypothesis"),
    }

    return {
        "id": golden_id,
        "component": "F",
        "task_type": "falsification_pairwise",
        "prompt": {
            "hypothesis": inp["hypothesis"],
            "evidence": inp["evidence"],
        },
        "response_A": inp["response_A"],
        "response_B": inp["response_B"],
        "preference": label["preference"],
        "rationale": label["F_rationale"],
        "extra": extra,
    }

def jsonl_write(path: Path, records):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False))
            f.write("\n")

def main():
    print("ğŸ” Converting golden examples â†’ train format ...")

    m_records = []
    d_records = []
    f_records = []

    # M
    for path in sorted((GOLDEN_DIR / "M").glob("*.json")):
        with path.open("r", encoding="utf-8") as f:
            instance = json.load(f)
        ok, msg = validate_golden(instance)
        if not ok:
            raise ValueError(f"[M] Invalid golden {path}: {msg}")
        m_records.append(convert_M(instance))

    # D
    for path in sorted((GOLDEN_DIR / "D").glob("*.json")):
        with path.open("r", encoding="utf-8") as f:
            instance = json.load(f)
        ok, msg = validate_golden(instance)
        if not ok:
            raise ValueError(f"[D] Invalid golden {path}: {msg}")
        d_records.append(convert_D(instance))

    # F
    for path in sorted((GOLDEN_DIR / "F").glob("*.json")):
        with path.open("r", encoding="utf-8") as f:
            instance = json.load(f)
        ok, msg = validate_golden(instance)
        if not ok:
            raise ValueError(f"[F] Invalid golden {path}: {msg}")
        f_records.append(convert_F(instance))

    # JSONLë¡œ ì €ì¥
    jsonl_write(TRAIN_DIR / "train_M.jsonl", m_records)
    jsonl_write(TRAIN_DIR / "train_D.jsonl", d_records)
    jsonl_write(TRAIN_DIR / "train_F.jsonl", f_records)

    print(f"âœ… M: {len(m_records)} â†’ {TRAIN_DIR/'train_M.jsonl'}")
    print(f"âœ… D: {len(d_records)} â†’ {TRAIN_DIR/'train_D.jsonl'}")
    print(f"âœ… F: {len(f_records)} â†’ {TRAIN_DIR/'train_F.jsonl'}")
    print("ğŸ‰ Done. ì´ì œ ì´ JSONLì„ ë°”ë¡œ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.")

if __name__ == "__main__":
    main()

```

---

## 2ï¸âƒ£ Label Studio export â†’ 2ì°¨ ê²€ì¦ validator

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:

1. Label Studio export JSON ë¡œë“œ
2. ìš°ë¦¬ê°€ ì •ì˜í•œ í…œí”Œë¦¿(í…ìŠ¤íŠ¸ ì˜ì—­ ì´ë¦„) ê¸°ì¤€ìœ¼ë¡œ:
    - M: `explicit_premises`, `implicit_premises`, `suspect_flags`, `m_score`, `m_rationale`
    - D/F: `preference`, `rationale`
3. ê° annotationì„ **â€œgoldenê³¼ ë™ì¼í•œ êµ¬ì¡°â€**ë¡œ ë³€í™˜
4. ì´ë¯¸ ë§Œë“¤ì–´ë‘” `schema_golden_*.json`ìœ¼ë¡œ validate
    
    â†’ êµ¬ì¡°/ìŠ¤ì½”ì–´ ë²”ìœ„/í•„ë“œ ëˆ„ë½ ëª¨ë‘ ì¡ëŠ” 2ì°¨ í•„í„°
    

ì´ë ‡ê²Œ í•˜ë©´:

- Annotatorë“¤ì´ Label Studioì—ì„œ ì´ìƒí•˜ê²Œ ì…ë ¥í•œ ì¼€ì´ìŠ¤
- ì ìˆ˜ ë²”ìœ„ ì˜ëª», suspect_flags í˜•ì‹ ì´ìƒ ë“±

ì„ ì „ë¶€ ìë™ìœ¼ë¡œ ê±¸ëŸ¬ë‚¼ ìˆ˜ ìˆë‹¤.

```python
#!/usr/bin/env python3
"""
validate_labelstudio_annotations.py

Label Studio export JSONì„ ì½ì–´ì„œ:
  1) M/D/F ì‘ì—…ì„ golden-like internal êµ¬ì¡°ë¡œ ë³€í™˜
  2) ê¸°ì¡´ schema_golden_*.jsonìœ¼ë¡œ validate (2ì°¨ ê²€ì¦)

Usage:
    python scripts/validate_labelstudio_annotations.py \
        --input data/annotations/ls_export.json

ì£¼ì˜:
  - Label Studio í…œí”Œë¦¿ì€ ìš°ë¦¬ê°€ ì œì•ˆí•œ XML ê¸°ì¤€
    (explicit_premises / implicit_premises / suspect_flags / m_score / m_rationale / preference / rationale)
"""

import json
import argparse
from pathlib import Path
from typing import Dict, Any, List, Tuple

import jsonschema
from jsonschema import Draft7Validator

ROOT_DIR = Path(__file__).resolve().parents[1]
SCHEMA_DIR = ROOT_DIR / "schemas"

def load_schema(name: str) -> Dict[str, Any]:
    path = SCHEMA_DIR / name
    if not path.exists():
        raise FileNotFoundError(f"Schema file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

SCHEMA_M = load_schema("schema_golden_M_premise_purification.json")
SCHEMA_D = load_schema("schema_golden_D_deduction_pairwise.json")
SCHEMA_F = load_schema("schema_golden_F_falsification_pairwise.json")

VALIDATORS = {
    ("M", "premise_purification"): Draft7Validator(SCHEMA_M),
    ("D", "deduction_pairwise"): Draft7Validator(SCHEMA_D),
    ("F", "falsification_pairwise"): Draft7Validator(SCHEMA_F),
}

def _split_lines(value: Any) -> List[str]:
    """Label Studio textarea â†’ line list ë¡œ normalize."""
    if isinstance(value, list) and value:
        # ë³´í†µ ["line1\nline2"] í˜•íƒœ
        text = value[0]
    elif isinstance(value, str):
        text = value
    else:
        return []
    return [line.strip() for line in text.splitlines() if line.strip()]

def _parse_suspect_flags(lines: List[str]) -> List[Dict[str, Any]]:
    """
    suspect_flags textarea:
      "0: ê²½í—˜ì ìœ¼ë¡œ ì˜ì‹¬ - ..." í˜•ì‹ â†’ [{"premise_index": 0, "reason": "..."}]
    """
    results = []
    for line in lines:
        if ":" not in line:
            continue
        idx_str, reason = line.split(":", 1)
        idx_str = idx_str.strip()
        reason = reason.strip()
        try:
            idx = int(idx_str)
        except ValueError:
            # ì¸ë±ìŠ¤ íŒŒì‹± ì‹¤íŒ¨ â†’ ìŠ¤í‚µ or log
            continue
        if reason:
            results.append({"premise_index": idx, "reason": reason})
    return results

def ls_task_to_golden_M(task: Dict[str, Any], ann: Dict[str, Any]) -> Dict[str, Any]:
    """
    Label Studio M task + annotation â†’ golden-like M object
    (schema_golden_M_premise_purification.json ì¤€ìˆ˜)
    """
    data = task.get("data", {})
    results = ann.get("result", [])

    # resultë“¤ì€ name í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì°¾ì•„ì„œ ë§¤í•‘
    value_map = {}
    for r in results:
        name = r.get("from_name") or r.get("name")
        if not name:
            continue
        value_map[name] = r.get("value", {})

    explicit_lines = _split_lines(value_map.get("explicit_premises", {}).get("text"))
    implicit_lines = _split_lines(value_map.get("implicit_premises", {}).get("text"))
    suspect_lines = _split_lines(value_map.get("suspect_flags", {}).get("text"))

    # premise_list êµ¬ì„±
    premise_list = []
    for t in explicit_lines:
        premise_list.append({"text": t, "source": "explicit"})
    for t in implicit_lines:
        premise_list.append({"text": t, "source": "implicit"})

    suspect_flags = _parse_suspect_flags(suspect_lines)

    # m_score: Rating â†’ 0~10 starë¥¼ 0~1ë¡œ normalize (í˜¹ì€ ê·¸ëƒ¥ 0~10 ê·¸ëŒ€ë¡œ ì¨ë„ ë˜ì§€ë§Œ
    # schemaëŠ” 0~1ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ê°„ë‹¨íˆ 10ìœ¼ë¡œ ë‚˜ëˆˆë‹¤)
    m_score_raw = value_map.get("m_score", {}).get("rating")
    if m_score_raw is None:
        m_score = 0.0
    else:
        m_score = float(m_score_raw) / 10.0

    m_rationale = ""
    if "m_rationale" in value_map:
        texts = value_map["m_rationale"].get("text")
        lines = _split_lines(texts)
        m_rationale = "\n".join(lines)

    golden_like = {
        "golden_id": str(task.get("id", data.get("item_id", "M_unknown"))),
        "component": "M",
        "task_type": "premise_purification",
        "input": {
            "question": data.get("question", ""),
            "context": data.get("context", ""),
        },
        "model_response": data.get("model_response", ""),
        "golden_label": {
            "premise_list": premise_list,
            "suspect_flags": suspect_flags,
            "M_score": m_score,
            "M_rationale": m_rationale,
        },
        "annotation_notes": f"from_label_studio_annotator_{ann.get('completed_by')}",
    }

    return golden_like

def ls_task_to_golden_D_or_F(
    task: Dict[str, Any],
    ann: Dict[str, Any],
    component: str,
) -> Dict[str, Any]:
    """
    Label Studio D/F task â†’ golden-like D/F object.

    data í•„ë“œ ì˜ˆ:
      component: "D" or "F"
      task_type: "deduction_pairwise" / "falsification_pairwise"
      premises/hypothesis/evidence/response_A/response_B ...

    annotation:
      preference (Choices), rationale (TextArea)
    """
    data = task.get("data", {})
    results = ann.get("result", [])

    value_map = {}
    for r in results:
        name = r.get("from_name") or r.get("name")
        if not name:
            continue
        value_map[name] = r.get("value", {})

    # preference: Choices â†’ value list ì¤‘ ì²« ë²ˆì§¸
    pref_vals = value_map.get("preference", {}).get("choices", [])
    preference = pref_vals[0] if pref_vals else None

    # rationale
    rationale_lines = _split_lines(value_map.get("rationale", {}).get("text"))
    rationale = "\n".join(rationale_lines)

    task_type = data.get("task_type")
    if component == "D":
        golden_like = {
            "golden_id": str(task.get("id", data.get("item_id", "D_unknown"))),
            "component": "D",
            "task_type": task_type or "deduction_pairwise",
            "input": {
                "premises": data.get("premises", ""),
                "hypothesis": data.get("hypothesis", ""),
                "response_A": data.get("response_A", ""),
                "response_B": data.get("response_B", ""),
            },
            "golden_label": {
                "preference": preference,
                "D_rationale": rationale,
            },
            "annotation_notes": f"from_label_studio_annotator_{ann.get('completed_by')}",
        }
    else:  # F
        golden_like = {
            "golden_id": str(task.get("id", data.get("item_id", "F_unknown"))),
            "component": "F",
            "task_type": task_type or "falsification_pairwise",
            "input": {
                "hypothesis": data.get("hypothesis", ""),
                "evidence": data.get("evidence", ""),
                "response_A": data.get("response_A", ""),
                "response_B": data.get("response_B", ""),
            },
            "golden_label": {
                "preference": preference,
                "F_rationale": rationale,
            },
            "annotation_notes": f"from_label_studio_annotator_{ann.get('completed_by')}",
        }

    return golden_like

def validate_with_schema(instance: Dict[str, Any]) -> Tuple[bool, str]:
    comp = instance.get("component")
    task_type = instance.get("task_type")
    key = (comp, task_type)

    if key not in VALIDATORS:
        return False, f"Unsupported component/task_type: {comp}/{task_type}"

    validator = VALIDATORS[key]
    errors = sorted(validator.iter_errors(instance), key=lambda e: e.path)
    if errors:
        msgs = []
        for err in errors:
            loc = " -> ".join(str(x) for x in err.path) or "(root)"
            msgs.append(f"At {loc}: {err.message}")
        return False, "; ".join(msgs)
    return True, ""

def main():
    parser = argparse.ArgumentParser(
        description="Validate Label Studio export using golden schemas"
    )
    parser.add_argument(
        "--input", required=True, help="Label Studio export JSON file"
    )
    parser.add_argument(
        "--output",
        default=None,
        help="(optional) ë³€í™˜ëœ golden-like annotationì„ ì €ì¥í•  JSONL ê²½ë¡œ",
    )
    args = parser.parse_args()

    path = Path(args.input)
    if not path.exists():
        raise FileNotFoundError(path)

    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    converted: List[Dict[str, Any]] = []
    total = 0
    invalid = 0

    for task in data:
        comp = task.get("data", {}).get("component")
        if not comp:
            # component ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë˜ëŠ” ë¡œê·¸)
            continue

        for ann in task.get("annotations", []):
            total += 1
            if comp == "M":
                inst = ls_task_to_golden_M(task, ann)
            elif comp in ("D", "F"):
                inst = ls_task_to_golden_D_or_F(task, ann, component=comp)
            else:
                print(f"[WARN] Unknown component {comp}, task id {task.get('id')}")
                continue

            ok, msg = validate_with_schema(inst)
            if not ok:
                invalid += 1
                print(f"[INVALID] task_id={task.get('id')} annotator={ann.get('completed_by')}")
                print(f"  - {msg}")
            else:
                converted.append(inst)

    print()
    print(f"ì´ annotation: {total}")
    print(f"ìœ íš¨(valid): {total - invalid}")
    print(f"ë¬´íš¨(invalid): {invalid}")

    if args.output:
        out_path = Path(args.output)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with out_path.open("w", encoding="utf-8") as f:
            for inst in converted:
                f.write(json.dumps(inst, ensure_ascii=False))
                f.write("\n")
        print(f"âœ… ë³€í™˜ëœ golden-like annotation {len(converted)}ê°œë¥¼ {out_path} ì— ì €ì¥")

    # ì‹¤íŒ¨ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ non-zero exit code
    if invalid > 0:
        print("âš ï¸ ì¼ë¶€ annotationì´ schemaë¥¼ í†µê³¼í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìœ„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit(2)
    else:
        print("ğŸ‰ ëª¨ë“  annotationì´ schema ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        exit(0)

if __name__ == "__main__":
    main()

```

---

## ğŸ”— ì •ë¦¬: íŒŒì´í”„ë¼ì¸ íë¦„

1. **ê³¨ë“  ì œì‘**
    - `golden_examples/M|D|F/*.json` (ì´ë¯¸ ìˆìŒ)
    - `scripts/validate_annotation.py golden_examples` â†’ êµ¬ì¡° OK í™•ì¸
2. **ê³¨ë“  â†’ í•™ìŠµ í¬ë§·**
    - `python scripts/convert_golden_to_train.py`
    - â†’ `data/train/train_M.jsonl`, `train_D.jsonl`, `train_F.jsonl`
3. **Label Studio ë¼ë²¨ë§**
    - export JSON: `data/annotations/ls_export_2025-11-14.json`
4. **Label Studio ê²°ê³¼ 2ì°¨ ê²€ì¦ + golden-like ë³€í™˜**
    - `python scripts/validate_labelstudio_annotations.py \ --input data/annotations/ls_export_2025-11-14.json \ --output data/annotations/ls_valid_2025-11-14.jsonl`
5. (ì˜µì…˜) `ls_valid_*.jsonl` â†’ train í¬ë§·ìœ¼ë¡œ í•©ì¹˜ëŠ” ì‘ì€ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€í•´ì„œ
    
    ê³¨ë“  + ì‹¤ annotation ì„ì–´ì„œ full DF-Mini íŒŒì¸íŠœë‹.